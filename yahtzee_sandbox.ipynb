{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_utils import (\n",
    "    ALL_ROLL_TUPLES,\n",
    "    Box,\n",
    "    BoxCategories,\n",
    "    RollAction,\n",
    "    RollValues,\n",
    "    ScoreAction,\n",
    "    ScoreCard,\n",
    "    roll_first,\n",
    "    remove_dice,\n",
    "    roll_again,\n",
    "    GameState,\n",
    "    ROLL_TUPLES_BY_BOX,\n",
    ")\n",
    "from agents import Agent, EpsilonGreedyAgent, GreedyAgent, RandomAgent\n",
    "from tabulate_best_actions import all_expected_scores_table, expected_scores_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state in the game consists of a `ScoreCard` (score card state), `RollValues` (values of dice showing on the table), and `rolls_completed` from 1 to 3 within the turn. The `ScoreCard` contains all needed information from the previous turns. The `RollValues` just contains the values of the five dice that have been rolled at a given point.\n",
    "\n",
    "`GameState` contains all three of these objects. It provides `possible_score_actions`, which gives the scores possible with a given set of dice values and score card state. I frame these as actions because at any time the player can choose to end their turn and score with one of these values. For convenience, they are sorted in descending order by score. `GameState` also provides `possible_actions`, which includes roll actions in addition to the `possible_score_actions`. The `re_roll` method takes dice that are specified by value and rolls again. Finally, `GameState` provides an `update_score` method, which updates the scorecard given a choice of box.\n",
    "\n",
    "The current score can be accessed at any time by calling the `score` method of the `GameState`'s `scorecard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent_scores = RandomAgent().play_games(n_games=10_000, histogram_bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score for a `RandomAgent` over 10,000 games is less than 50 - that's pretty bad! There is a long tail on the right, with a few scores above 100 being achieved.\n",
    "\n",
    "In this first pass at implementing the gameplay actions and scoring, I focused on development speed and cleanliness of the design. I deliberatly did not spend time optimizing for runtime speed, and it shows here, as it took several minutes to simulate 10,000 one-player games. I still think this is OK for now; I can optimize later once I start training some deep RL agents.\n",
    "\n",
    "Let's try a `GreedyAgent` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_agent_scores = GreedyAgent().play_games(n_games=1000, histogram_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for s in greedy_agent_scores if s > 150]) / len(greedy_agent_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GreedyAgent` does substantially better: it achieves mean score above 100, in about 5% of games it scores better than 150, and in a few games it even gets close to 200. \n",
    "\n",
    "Still, the performance is not great compared to typcial human gameplay. I'm not a terribly skilled player, and my own threshold for a good game is a score around 200.\n",
    "\n",
    "It's pretty clear that a naive $\\epsilon$-greedy agent won't do better. Since dice have no memory, there's no advantage of sometimes taking a `RollAction` instead of the best `ScoreAction`; to do better, we'd have to pick our `RollAction` strategically, not at random. Trying out the naive agent confirms that it's not a better approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy_agent_scores = EpsilonGreedyAgent(epsilon=0.5).play_games(n_games=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll try to come up with a reasonably good strategy, similar to the one I try to follow when playing in the real world. Roughly speaking, given the five dice values after the first roll I pick an unused box to \"go for\" and then roll the dice that maximize the probability of hitting the box (or scoring as high as possible in the box, for the upper section) on this turn. For now let's set aside the question of how to pick which box to go for, and focus on figuring out which dice to roll to hit the box.\n",
    "\n",
    "Given `roll_values` and `box`, I need to figure out the `roll_action` for that box, which I define as the one that maximizes the expected score *from that box* when `roll_action` is taken. Note that I am not computing the total expected score from the `roll_action` on `roll_values`, just the expected score that comes from the particular box. The idea is that my agent will simply look at the dice and either take a `ScoreAction` or pick a box to \"roll for\". The advantage of this is that it places a smaller limit on the number of possible actions than considering all possible roll actions. However, there may be cases where this is suboptimal: say `roll_action_1` is the best to hit `SmallStraight` and `roll_action_2` is the best to hit `FullHouse`, but there is some other action `roll_action_3` that has higher sum of expected scores for `SmallStraight` and `FullHouse`. I'll neglect such cases for this agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out the best `RollAction` for a small or large straight is not as straightforward (ha) as for the other categories. Given particular `RollValues` it's usually pretty easy to see what to roll, but the casework here could make the function implementation a bit messy. Instead of attempting to write out all the cases, I just precomputed a map from the roll values to the best roll action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_scores_table().loc[(1, 1, 1, 2, 2), :].loc[Box.FullHouse.name][\"dice_values_to_roll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_roll_action_for_box_with_score(roll_values: RollValues, box: Box) -> Tuple[Optional[RollAction], float]:\n",
    "    \"\"\"\n",
    "    Given a set of dice values and a box, returns the dice to roll in order to maximize\n",
    "    the expected value of the score in the box, and also gives the expecte score. If\n",
    "    rolling no dice gives the best score, then None is returned as the roll action.\n",
    "    \"\"\"\n",
    "    row = expected_scores_table().loc[roll_values.values, :].loc[box.name]\n",
    "    roll_action_tuple, score = row[\"dice_values_to_roll\"], row[\"expected_score\"]\n",
    "    roll_action = RollAction(*roll_action_tuple) if len(roll_action_tuple) != 0 else None\n",
    "    return roll_action, score\n",
    "\n",
    "def best_roll_action_for_box(roll_values: RollValues, box: Box) -> Optional[RollAction]:\n",
    "    \"\"\"\n",
    "    Given a set of dice values and a box, returns the dice to roll in order to maximize\n",
    "    the expected value of the score in the box. If rolling no dice gives the best score,\n",
    "    then None is returned.\n",
    "    \"\"\"\n",
    "    return best_roll_action_for_box_with_score(roll_values, box)[0]\n",
    "\n",
    "def best_action_by_box_with_score(roll_values: RollValues, allowed_boxes: Set[Box]) -> Dict[Box, Tuple[Union[RollAction, ScoreAction], float]]:\n",
    "    result = {}\n",
    "    for box in allowed_boxes:\n",
    "        best_roll_action, score = best_roll_action_for_box_with_score(roll_values, box)\n",
    "        best_action = best_roll_action if best_roll_action is not None else ScoreAction(roll_values.score_from_box(box), roll_values.values, box)\n",
    "        result[box] = (best_action, score)\n",
    "    return result\n",
    "\n",
    "def greedy_best_action(roll_values: RollValues, allowed_boxes: Set[Box]) -> Union[RollAction, ScoreAction]:\n",
    "    return max(best_action_by_box_with_score(roll_values, allowed_boxes).values(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = best_action_by_box_with_score(RollValues(1, 1, 2, 2, 2), {box for box in Box})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = GameState()\n",
    "game_state.start_turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = greedy_best_action(game_state.roll_values, game_state.scorecard.unused_boxes)\n",
    "print(action)\n",
    "game_state.take_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyExpectedScoresAgent(Agent):\n",
    "    \"\"\"\n",
    "    After each of the first two rolls in a turn, this agent chooses the action that\n",
    "    has the highest expected score for some box. After the third roll, it simply\n",
    "    selects the ScoreAction with the highest score (if a ScoreAction has not already\n",
    "    been taken on that turn).\n",
    "\n",
    "    Note that this agent does not necessarily take the action with the highest expected\n",
    "    score, just with the highest expected score for some box. You should think of this\n",
    "    agent as choosing a \"box action\" from the available boxes: it looks at the boxes,\n",
    "    determines the action that maximizes the expected score from each, and then out of\n",
    "    those actions picks the max.\n",
    "    \"\"\"\n",
    "\n",
    "    def choose_action(self, game_state: GameState) -> Union[RollAction, ScoreAction]:\n",
    "        if game_state.rolls_completed < 3:\n",
    "            return greedy_best_action(game_state.roll_values, game_state.scorecard.unused_boxes)\n",
    "        else:\n",
    "            return game_state.sorted_possible_score_actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GreedyExpectedScoresAgent(narrate=True).play_single_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_expected_scores_agent_scores = GreedyExpectedScoresAgent().play_games(n_games=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_expected_scores_agent_scores_2 = GreedyExpectedScoresAgent().play_games(n_games=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in greedy_expected_scores_agent_scores_2 if x > 250]) / len(greedy_expected_scores_agent_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(greedy_expected_scores_agent_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent does much better. In 10,000 games, it got a median score of 197 and mean score of 204, and it scored over 250 more than 10% of the time. Here's a histogram of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results](greedy_expected_scores_agent_10_000_games_hist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(greedy_expected_scores_agent_scores_2, bins=50)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.title(\"Scores from 10000 Yahtzee games with GreedyExpectedScoresAgent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
