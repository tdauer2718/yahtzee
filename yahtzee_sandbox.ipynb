{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_utils import (\n",
    "    ALL_ROLL_TUPLES,\n",
    "    Box,\n",
    "    BoxCategories,\n",
    "    RollAction,\n",
    "    RollValues,\n",
    "    ScoreAction,\n",
    "    ScoreCard,\n",
    "    roll_first,\n",
    "    remove_dice,\n",
    "    roll_again,\n",
    "    GameState,\n",
    "    ROLL_TUPLES_BY_BOX,\n",
    ")\n",
    "from agents import Agent, EpsilonGreedyAgent, GreedyAgent, RandomAgent\n",
    "from expected_score_utils import (\n",
    "    all_expected_scores_table_by_box,\n",
    "    best_expected_scores_table_by_box,\n",
    "    best_roll_action_for_box_with_score,\n",
    "    best_action_by_box_with_score,\n",
    "    greedy_best_action,\n",
    "    hit_probability_from_action,\n",
    "    create_expected_scores_table_two_rolls,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state in the game consists of a `ScoreCard` (score card state), `RollValues` (values of dice showing on the table), and `rolls_completed` from 1 to 3 within the turn. The `ScoreCard` contains all needed information from the previous turns. The `RollValues` just contains the values of the five dice that have been rolled at a given point.\n",
    "\n",
    "`GameState` contains all three of these objects. It provides `possible_score_actions`, which gives the scores possible with a given set of dice values and score card state. I frame these as actions because at any time the player can choose to end their turn and score with one of these values. For convenience, they are sorted in descending order by score. `GameState` also provides `possible_actions`, which includes roll actions in addition to the `possible_score_actions`. The `re_roll` method takes dice that are specified by value and rolls again. Finally, `GameState` provides an `update_score` method, which updates the scorecard given a choice of box.\n",
    "\n",
    "The current score can be accessed at any time by calling the `score` method of the `GameState`'s `scorecard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = GameState()\n",
    "game_state.start_turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = greedy_best_action(game_state.roll_values, game_state.scorecard.unused_boxes, 3 - game_state.rolls_completed)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state.take_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = greedy_best_action(game_state.roll_values, game_state.scorecard.unused_boxes, 3 - game_state.rolls_completed)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state.take_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = greedy_best_action(game_state.roll_values, game_state.scorecard.unused_boxes, 3 - game_state.rolls_completed)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahtzeeDeepDoubleQ(Agent):\n",
    "    \"\"\"\n",
    "    Using the MarioAgent here as a template:\n",
    "    https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, save_dir, narrate: bool = False):\n",
    "        super().__init__(narrate=narrate)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = YahtzeeNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def choose_action(self, game_state: GameState):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = game_state.to_array()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
